{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AUDIO SIGNAL ANALYSIS USING MACHINE LEARNING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\programdata\\anaconda3\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (1.11.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: keras in c:\\programdata\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow in c:\\programdata\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (1.24.3)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (0.58.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (1.8.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (4.7.1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (0.41.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa scipy scikit-learn keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Virtualize normal/abnormal heart sounds and their corresponding MFCCs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import librosa, librosa.display, IPython.display, time, pywt\n",
    "import numpy as np, pandas as pd, keras.backend as K, matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.callbacks import Callback\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'librosa.display' has no attribute 'waveplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m normal_x, normal_fs \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msneha\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mheart-sound-classification\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ma0007.wav\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m normal_x \u001b[38;5;241m=\u001b[39m normal_x[:\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m*\u001b[39mnormal_fs]\n\u001b[1;32m----> 6\u001b[0m librosa\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mwaveplot(normal_x, sr\u001b[38;5;241m=\u001b[39mnormal_fs)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormal signal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'librosa.display' has no attribute 'waveplot'"
     ]
    }
   ],
   "source": [
    "## Display a normal heart sound and an abnormal heart sound\n",
    "\n",
    "# Load and display normal file\n",
    "normal_x, normal_fs = librosa.load(r'C:\\Users\\sneha\\heart-sound-classification\\training\\normal\\a0007.wav')\n",
    "normal_x = normal_x[:5*normal_fs]\n",
    "librosa.display.waveshow(normal_x, sr=normal_fs)\n",
    "plt.title('Normal signal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "IPython.display.display(IPython.display.Audio(normal_x, rate=normal_fs))\n",
    "\n",
    "# Load and display abnormal file\n",
    "abnormal_x, abnormal_fs = librosa.load(r'C:\\Users\\sneha\\heart-sound-classification\\training\\abnormal\\a0001.wav')\n",
    "abnormal_x = abnormal_x[:5*abnormal_fs]\n",
    "librosa.display.waveshow(abnormal_x, sr=abnormal_fs)\n",
    "plt.title('Abnormal signal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "IPython.display.display(IPython.display.Audio(abnormal_x, rate=abnormal_fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Virtualize MFCCs of a normal and an abnormal audio\n",
    "\n",
    "# Get MFCCs of a normal signal and display\n",
    "normal_mfccs = librosa.feature.mfcc(normal_x, sr=normal_fs)\n",
    "librosa.display.specshow(normal_mfccs, sr=normal_fs, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCCs of normal signal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Get MFCCs of an abnormal signal and display\n",
    "abnormal_mfccs = librosa.feature.mfcc(abnormal_x, sr=abnormal_fs)\n",
    "librosa.display.specshow(abnormal_mfccs, sr=abnormal_fs, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCCs of abnormal signal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Scale the MFCCs such that each coefficient dimension\n",
    "# has zero mean and unit variance\n",
    "\n",
    "# Normal signal\n",
    "normal_mfccs = scale(normal_mfccs, axis=1)\n",
    "librosa.display.specshow(normal_mfccs, sr=normal_fs, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('Scaled MFCCs of normal signal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Abnormal signal\n",
    "abnormal_mfccs = scale(abnormal_mfccs, axis=1)\n",
    "librosa.display.specshow(abnormal_mfccs, sr=abnormal_fs, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('Scaled MFCCs of abnormal signal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reduce MFCCs to 2D by tSNE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to read dataset and extract features\n",
    "\n",
    "def extract_feature(file,feat='mfcc',flt=True,nMFCC=96,flatten=True,mean=False):\n",
    "    # nMFCC, flatten, mean only apply when feat='mfcc'\n",
    "    \n",
    "    def butter_bandpass_filter(data, fs, lowcut=25, highcut=400, order=5):\n",
    "        nyq = 0.5*fs\n",
    "        low = lowcut/nyq\n",
    "        high = highcut/nyq\n",
    "        b, a = butter(order, [low, high], btype='band')\n",
    "        y = lfilter(b, a, data)\n",
    "        return y\n",
    "\n",
    "    # Load data and pre-process\n",
    "    data, rate = librosa.load(file, sr=None)\n",
    "    data = data[:5*rate]\n",
    "    \n",
    "    if flt:\n",
    "        data = butter_bandpass_filter(data, rate)\n",
    "    \n",
    "    if feat == 'mfcc':\n",
    "        if flatten:\n",
    "            return librosa.feature.mfcc(y=data,sr=rate,n_mfcc=nMFCC).flatten()\n",
    "        else:\n",
    "            data = librosa.resample(data, rate, 22050)\n",
    "            mfcc = librosa.feature.mfcc(y=data, n_mfcc=nMFCC)\n",
    "            if mean:\n",
    "                return np.mean(mfcc.T,axis=0)\n",
    "            else:\n",
    "                return mfcc\n",
    "            \n",
    "    elif feat == 'spamp':\n",
    "        return abs(np.fft.rfft(data))\n",
    "    \n",
    "    elif feat == 'wavelet':\n",
    "        cA, cD = pywt.dwt(data, 'db1')\n",
    "        return cD\n",
    "    \n",
    "    elif feat == 'mfcc-dwt':\n",
    "        data = librosa.resample(data, rate, 22050)\n",
    "        mfcc = np.mean((librosa.feature.mfcc(y=data, n_mfcc=nMFCC)).T,axis=0)\n",
    "        cA, cD = pywt.dwt(data, 'db1')\n",
    "        return np.concatenate((mfcc, cD))\n",
    "    \n",
    "    elif feat == 'raw':\n",
    "        return data\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('Invalid second argument')\n",
    "        \n",
    "        \n",
    "def read_dataset(dataset,feat='mfcc',flt=True,nMFCC=96,flatten=True,mean=False):\n",
    "    def read_dir(dir_path,feat='mfcc',flt=True,nMFCC=96,flatten=True,mean=False):\n",
    "        # Read REFERENCE.csv\n",
    "        if dir_path[-1] == '/':\n",
    "            dir_path = dir_path[:-1]\n",
    "        csv_path = dir_path + '/REFERENCE.csv'\n",
    "        df = pd.read_csv(csv_path, header=None)\n",
    "        features = []\n",
    "        labels = []\n",
    "        N = df.shape[0]\n",
    "\n",
    "        for i in range(N):\n",
    "            wav_path = dir_path +'/' + df.iat[i,0] + '.wav'\n",
    "            features.append(extract_feature(wav_path,feat,flt,nMFCC,flatten,mean))\n",
    "            if df.iat[i,1] == 1:\n",
    "                labels.append(1) # abnormal\n",
    "            else:\n",
    "                labels.append(0) # normal\n",
    "\n",
    "        return features, labels\n",
    "    \n",
    "    if dataset[-1] == '/':\n",
    "        dataset = dataset[:-1]\n",
    "       \n",
    "    # Read dataset\n",
    "    data_a, labels_a = read_dir(dataset+'/training-a',feat,flt,nMFCC,flatten,mean)\n",
    "    data_b, labels_b = read_dir(dataset+'/training-b',feat,flt,nMFCC,flatten,mean)\n",
    "    data_c, labels_c = read_dir(dataset+'/training-c',feat,flt,nMFCC,flatten,mean)\n",
    "    data_d, labels_d = read_dir(dataset+'/training-d',feat,flt,nMFCC,flatten,mean)\n",
    "    data_e, labels_e = read_dir(dataset+'/training-e',feat,flt,nMFCC,flatten,mean)\n",
    "    data_f, labels_f = read_dir(dataset+'/training-f',feat,flt,nMFCC,flatten,mean)\n",
    "    \n",
    "    data = np.concatenate((data_a, data_b, data_c, data_d, \\\n",
    "                            data_e, data_f))\n",
    "    labels = np.concatenate((labels_a, labels_b, labels_c, labels_d, \\\n",
    "                             labels_e, labels_f))\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reduce MFCCs to 2D by TSNE\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# read dataset\n",
    "mfcc, label = read_dataset(r'C:\\Users\\sneha\\heart-sound-classification\\training')\n",
    "\n",
    "# compile model\n",
    "tsne_model = TSNE().fit_transform(mfcc)\n",
    "\n",
    "# Virtualize model\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(tsne_model[label==1,0], tsne_model[label==1,1])\n",
    "\n",
    "plt.scatter(tsne_model[label==0,0], tsne_model[label==0,1])\n",
    "\n",
    "plt.legend(['abnormal', 'normal'])\n",
    "#plt.savefig('./result.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Heart sound classification: KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_KNN(features, labels, n=5):\n",
    "    \n",
    "    # Split training and validation sets\n",
    "    train_feat,test_feat,train_lab,test_lab = train_test_split(features,\\\n",
    "                                                               labels, test_size=0.3)     \n",
    "    \n",
    "    # Train model\n",
    "    model = KNeighborsClassifier(n_neighbors=n).fit(train_feat, train_lab)\n",
    "\n",
    "    # Predict the response for test dataset\n",
    "    test_pred = model.predict(test_feat)\n",
    "\n",
    "    # Model Accuracy, Precision, Recall, F score\n",
    "    accuracy = accuracy_score(test_lab, test_pred)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(\\\n",
    "                        test_lab, test_pred, average='binary')\n",
    "    print(\"\\tAccuracy:\\t\", round(accuracy, 2))\n",
    "    print(\"\\tPrecision:\\t\", round(precision, 2))\n",
    "    print(\"\\tRecall:\\t\\t\", round(recall, 2))\n",
    "    print(\"\\tF score:\\t\", round(fscore, 2))\n",
    "    return model, accuracy, precision, recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KNN using 96 MFCCs as feature\n",
    "print(\"KNN with 96 MFCCs\")\n",
    "model_mfcc96, acc_mfcc96, p_mfcc96, r_mfcc96, fscore_mfcc96 \\\n",
    "= train_KNN(mfcc, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KNN using 40 mean MFCCs as feature\n",
    "print(\"KNN with 40 mean MFCCs\")\n",
    "mfcc40, label = read_dataset('./',flt=False,nMFCC=40,flatten=False,mean=True)\n",
    "model_mfcc40, acc_mfcc40, p_mfcc40, r_mfcc40, fscore_mfcc40 \\\n",
    "= train_KNN(mfcc40, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KNN using spectral amplitude as feature\n",
    "print(\"KNN with spectral amplitudes\")\n",
    "spamp, label = read_dataset('./', feat='spamp')\n",
    "model_spamp, acc_spamp, p_spamp, r_spamp, fscore_spamp\\\n",
    "= train_KNN(spamp, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KNN using wavelet features\n",
    "print(\"KNN with wavelet features\")\n",
    "wavelet, label = read_dataset('./', feat='wavelet')\n",
    "model_wavelet, acc_wavelet, p_wavelet, r_wavelet, fscore_wavelet\\\n",
    "= train_KNN(wavelet, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KNN using both MFCC and wavelet features\n",
    "print(\"KNN with both 40 mean MFCCs and DWT\")\n",
    "mfcc_dwt, label = read_dataset('./', feat='mfcc-dwt')\n",
    "model_all, acc_all, p_all, r_all, fscore_all = train_KNN(mfcc_dwt, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Accuracy, Precision, Recall and F1 score\n",
    "\n",
    "# data to plot\n",
    "n_groups = 5\n",
    "objects = ('MFCC96', 'mean-MFCC40', 'Amp', 'Wavelet', 'MFCC-DWT')\n",
    "A = (acc_mfcc96,acc_mfcc40,acc_spamp,acc_wavelet,acc_all)\n",
    "P = (p_mfcc96,p_mfcc40,p_spamp,p_wavelet,p_all)\n",
    "R = (r_mfcc96,r_mfcc40,r_spamp,r_wavelet,r_all)\n",
    "F = (fscore_mfcc96,fscore_mfcc40,fscore_spamp,fscore_wavelet,fscore_all)\n",
    "\n",
    "# create plot\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.2\n",
    "opacity = 0.8\n",
    "\n",
    "rects1 = plt.bar(index, P, bar_width, \\\n",
    "                 alpha=opacity, color='b', label='Precision')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, R, bar_width, \\\n",
    "                 alpha=opacity, color='g', label='Recall')\n",
    "\n",
    "rects3 = plt.bar(index + bar_width*2, F, bar_width, \\\n",
    "                 alpha=opacity, color='r', label='F1 score')\n",
    "\n",
    "rects4 = plt.bar(index + bar_width*3, A, bar_width, \\\n",
    "                 alpha=opacity, color='y', label='Accuracy')\n",
    "\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Score')\n",
    "plt.title('KNN\\'s Accuracy, Precision, Recall and F1 score')\n",
    "plt.xticks(index + bar_width*2, objects)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('KNN.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN with mean 40-MFCC has the highest performance. Therefore, we try to optimize KNN model with these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc40, label = read_dataset('./',flt=False,nMFCC=40,flatten=False,mean=True)\n",
    "X_train,X_test,y_train,y_test = train_test_split(mfcc40, label, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search to optimize hyperparameter\n",
    "parameters = {'n_neighbors':[3,5,7,9,11,13],\n",
    "             'weights':['uniform','distance'],\n",
    "              'algorithm':['auto','ball_tree','kd_tree'],\n",
    "              'p':[1,2,3],\n",
    "              'n_jobs':[-1]\n",
    "             }\n",
    "\n",
    "model = GridSearchCV(KNeighborsClassifier(), param_grid=parameters)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "print('Best Score:\\t', model.best_score_)\n",
    "print('Best Params:\\t', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply these hyperparameters\n",
    "print(\"KNN with 40 mean MFCCs (optimized)\")\n",
    "    \n",
    "# Train model\n",
    "knn = KNeighborsClassifier(n_neighbors=3, weights='distance', algorithm='auto',\n",
    "                           p=1, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model Accuracy, Precision, Recall, F score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='binary')\n",
    "print(\"\\tAccuracy:\\t\", round(accuracy, 2))\n",
    "print(\"\\tPrecision:\\t\", round(precision, 2))\n",
    "print(\"\\tRecall:\\t\\t\", round(recall, 2))\n",
    "print(\"\\tF score:\\t\", round(fscore, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Heart sound classification: CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = np.asarray(self.model.predict(self.validation_data[0])).round()\n",
    "        val_predict = np.argmax(val_predict, axis=1)\n",
    "        val_targ = np.argmax(self.validation_data[1], axis=1)\n",
    "        _val_precision, _val_recall, _val_f1, _ = \\\n",
    "        precision_recall_fscore_support(val_targ, val_predict, average='binary')\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(\" — val_f1: %f — val_precision: %f — val_recall: %f\" \\\n",
    "              %(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "\n",
    "    \n",
    "def prepare_2D(X, Y):\n",
    "    X = X.reshape(X.shape[0],X.shape[1],X.shape[2],1)\n",
    "    Y = to_categorical(Y)\n",
    "    # split training and validation sets\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.3)\n",
    "    xtest, xvalid, ytest, yvalid = train_test_split(xtest, ytest, test_size=0.5)\n",
    "    \n",
    "    return xtrain, xvalid, xtest, ytrain, yvalid, ytest\n",
    "\n",
    "\n",
    "def prepare_1D(X, Y):\n",
    "    X = X.reshape(X.shape[0],X.shape[1],1)\n",
    "    Y = to_categorical(Y)\n",
    "    \n",
    "    # split training and validation sets\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.3)\n",
    "    xtest, xvalid, ytest, yvalid = train_test_split(xtest, ytest, test_size=0.5)\n",
    "    \n",
    "    return xtrain, xvalid, xtest, ytrain, yvalid, ytest\n",
    "\n",
    "\n",
    "    \n",
    "def plot_history(history, name='CNN'):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    axs[1].plot(history.history['loss'])\n",
    "    axs[1].plot(history.history['val_loss'])\n",
    "    axs[1].set_title(name+' loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # Plot training & validation fscore values\n",
    "    axs[0].plot(history.history['acc'])\n",
    "    axs[0].plot(history.history['val_acc'])\n",
    "    axs[0].set_title(name+' Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    fig.savefig(name+'-history.png')\n",
    "    \n",
    "    \n",
    "def plot_metrics(metrics,name='CNN'):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(25,5))\n",
    "    \n",
    "    # Plot validation fscore values\n",
    "    axs[0].plot(metrics.val_f1s)\n",
    "    axs[0].set_title(name+' Validation Fscore')\n",
    "    axs[0].set_ylabel('Fscore')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    \n",
    "    # Plot validation recall values\n",
    "    axs[1].plot(metrics.val_recalls)\n",
    "    axs[1].set_title(name+' Validation Recall')\n",
    "    axs[1].set_ylabel('Recall')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    \n",
    "    # Plot validation precision values\n",
    "    axs[2].plot(metrics.val_precisions)\n",
    "    axs[2].set_title(name+' Validation Precision')\n",
    "    axs[2].set_ylabel('Precision')\n",
    "    axs[2].set_xlabel('Epoch')\n",
    "    \n",
    "    fig.savefig(name+'-metrics.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2D CNN - 40 mean MFCCs\n",
    "mfcc402, label = read_dataset('./',flt=False,nMFCC=40,flatten=False)\n",
    "\n",
    "xtrain402,xvalid402,xtest402,ytrain402,yvalid402,ytest402 = prepare_2D(mfcc402,label)\n",
    "    \n",
    "# create model\n",
    "model402 = Sequential()\n",
    "\n",
    "# add model layers\n",
    "model402.add(Conv2D(16, kernel_size=2, activation='relu',\n",
    "                   input_shape=(xtrain402.shape[1],xtrain402.shape[2],1)))\n",
    "model402.add(MaxPooling2D(pool_size=2))\n",
    "model402.add(Dropout(0.2))\n",
    "\n",
    "model402.add(Conv2D(32, kernel_size=2, activation='relu'))\n",
    "model402.add(MaxPooling2D(pool_size=2))\n",
    "model402.add(Dropout(0.2))\n",
    "\n",
    "model402.add(Conv2D(64, kernel_size=2, activation='relu'))\n",
    "model402.add(MaxPooling2D(pool_size=2))\n",
    "model402.add(Dropout(0.2))\n",
    "\n",
    "model402.add(GlobalAveragePooling2D())\n",
    "model402.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "# compile\n",
    "model402.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model402.summary()\n",
    "\n",
    "# train model\n",
    "metrics402 = Metrics()\n",
    "history402 = model402.fit(xtrain402, ytrain402, validation_data=(xvalid402, yvalid402),\n",
    "                        epochs=30, callbacks=[metrics402], verbose=2)\n",
    "\n",
    "plot_history(history402, 'CNN-402')\n",
    "plot_metrics(metrics402, 'CNN-402')\n",
    "\n",
    "print(classification_report(np.argmax(ytest402,axis=1),\n",
    "                      np.argmax(model402.predict(xtest402),axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1D CNN - 40 mean MFCCs\n",
    "mfcc40, label = read_dataset('./',flt=False,nMFCC=40,flatten=False,mean=True)\n",
    "\n",
    "xtrain40,xvalid40,xtest40,ytrain40,yvalid40,ytest40 = prepare_1D(mfcc40,label)\n",
    "    \n",
    "# create model\n",
    "model40 = Sequential()\n",
    "\n",
    "# add model layers\n",
    "model40.add(Conv1D(16, kernel_size=2, activation='relu',\n",
    "                   input_shape=(xtrain40.shape[1],1)))\n",
    "model40.add(MaxPooling1D(pool_size=2))\n",
    "model40.add(Dropout(0.2))\n",
    "\n",
    "model40.add(Conv1D(32, kernel_size=2, activation='relu'))\n",
    "model40.add(MaxPooling1D(pool_size=2))\n",
    "model40.add(Dropout(0.2))\n",
    "\n",
    "model40.add(Conv1D(64, kernel_size=2, activation='relu'))\n",
    "model40.add(MaxPooling1D(pool_size=2))\n",
    "model40.add(Dropout(0.2))\n",
    "\n",
    "model40.add(Conv1D(128, kernel_size=2, activation='relu'))\n",
    "model40.add(MaxPooling1D(pool_size=2))\n",
    "model40.add(Dropout(0.2))\n",
    "\n",
    "model40.add(GlobalAveragePooling1D())\n",
    "model40.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "# compile\n",
    "model40.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model40.summary()\n",
    "\n",
    "# train model\n",
    "metrics40 = Metrics()\n",
    "history40 = model40.fit(xtrain40, ytrain40, validation_data=(xvalid40, yvalid40),\n",
    "                        epochs=30, callbacks=[metrics40], verbose=2)\n",
    "\n",
    "plot_history(history40, 'CNN-401')\n",
    "plot_metrics(metrics40, 'CNN-401')\n",
    "\n",
    "print(classification_report(np.argmax(ytest40,axis=1),\n",
    "                      np.argmax(model40.predict(xtest40),axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Heart sound classification: LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM - 40 mean MFCCs\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=64, dropout=0.05, recurrent_dropout=0.20, \n",
    "               return_sequences=True,input_shape = (40,1)))\n",
    "model.add(LSTM(units=32, dropout=0.05, recurrent_dropout=0.20,return_sequences=False))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adamax',metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "xtrain,xvalid,xtest,ytrain,yvalid,ytest = prepare_1D(mfcc40,label)\n",
    "\n",
    "metrics = Metrics()\n",
    "history = model.fit(xtrain, ytrain, validation_data=(xvalid, yvalid),\n",
    "                    epochs=100, callbacks=[metrics], verbose=2)\n",
    "\n",
    "plot_history(history, 'LSTM-40')\n",
    "plot_metrics(metrics, 'LSTM-40')\n",
    "\n",
    "print(classification_report(np.argmax(ytest,axis=1),\n",
    "                      np.argmax(model.predict(xtest),axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.utils import plot_model\n",
    "#plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dill\n",
    "#dill.dump_session('notebook_session.db')\n",
    "#import dill\n",
    "#dill.load_session('notebook_session.db')\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store mfcc402\n",
    "%store mfcc40\n",
    "%store label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "def data():\n",
    "    import librosa\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from keras.utils import to_categorical\n",
    "    def extract_feature(file):\n",
    "        # Load data and pre-process\n",
    "        data, rate = librosa.load(file)\n",
    "        data = data[:5*rate]\n",
    "        mfcc = librosa.feature.mfcc(y=data, n_mfcc=40)\n",
    "        return np.mean(mfcc.T,axis=0)\n",
    "\n",
    "    def read_dir(dir_path,feat='mfcc'):\n",
    "        # Read REFERENCE.csv\n",
    "        if dir_path[-1] == '/':\n",
    "            dir_path = dir_path[:-1]\n",
    "        csv_path = dir_path + '/REFERENCE.csv'\n",
    "        df = pd.read_csv(csv_path, header=None)\n",
    "        features = []\n",
    "        labels = []\n",
    "        N = df.shape[0]\n",
    "        for i in range(N):\n",
    "            wav_path = dir_path +'/' + df.iat[i,0] + '.wav'\n",
    "            features.append(extract_feature(wav_path))\n",
    "            if df.iat[i,1] == 1:\n",
    "                labels.append(1) # abnormal\n",
    "            else:\n",
    "                labels.append(0) # normal\n",
    "        return features, labels\n",
    "\n",
    "    def read_dataset(dataset):\n",
    "        if dataset[-1] == '/':\n",
    "            dataset = dataset[:-1]\n",
    "        # Read dataset\n",
    "        data_a, labels_a = read_dir(dataset+'/training-a')\n",
    "        data_b, labels_b = read_dir(dataset+'/training-b')\n",
    "        data_c, labels_c = read_dir(dataset+'/training-c')\n",
    "        data_d, labels_d = read_dir(dataset+'/training-d')\n",
    "        data_e, labels_e = read_dir(dataset+'/training-e')\n",
    "        data_f, labels_f = read_dir(dataset+'/training-f')\n",
    "        data = np.concatenate((data_a, data_b, data_c, data_d, \n",
    "                               data_e, data_f))\n",
    "        labels = np.concatenate((labels_a, labels_b, labels_c, \n",
    "                                 labels_d, labels_e, labels_f))\n",
    "        return data, labels\n",
    "\n",
    "    x, y = read_dataset('./')\n",
    "    x = x.reshape(x.shape[0],x.shape[1],1)\n",
    "    y = to_categorical(y)\n",
    "    \n",
    "    # split training and validation sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, LSTM\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(units={{choice([16,32,64,128,256])}},\n",
    "                   dropout={{uniform(0, 1)}},\n",
    "                   recurrent_dropout={{uniform(0, 1)}},\n",
    "                   return_sequences=True,\n",
    "                   input_shape = (40,1)))\n",
    "    \n",
    "    model.add(LSTM(units={{choice([16,32,64,128,256])}},\n",
    "                   dropout={{uniform(0, 1)}},\n",
    "                   recurrent_dropout={{uniform(0, 1)}},\n",
    "                   return_sequences=False))\n",
    "    \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer={{choice(['rmsprop', 'adam', 'sgd'])}},\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    result = model.fit(x_train, y_train,\n",
    "              batch_size={{choice([16,32,64,128])}},\n",
    "              epochs=2,\n",
    "              verbose=2,\n",
    "              validation_split=0.2)\n",
    "    \n",
    "    # get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=5,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='audio_analysis-Copy3')\n",
    "\n",
    "    print(\"Best params:\")\n",
    "    print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
